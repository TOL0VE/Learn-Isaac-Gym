import isaacgym


import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import datetime 
from torch.utils.tensorboard import SummaryWriter 

from env import CartPoleEnv
from model import CartPoleActorCritic

import os

# ==========================================
# Hyperparameters (è¶…å‚æ•°)
# ==========================================
MAX_ITERATIONS = 50000       # ç¨å¾®å¢åŠ ä¸€ç‚¹ï¼Œè®©ä½ èƒ½çœ‹åˆ°æ›´é•¿çš„æ›²çº¿
NUM_STEPS = 24     
MINI_BATCH_SIZE = 4096*2
NUM_ENVS = 4096*2      
SAVE_INTERVAL = 100          
LEARNING_RATE = 3e-4   

GAMMA = 0.99                
GAE_LAMBDA = 0.95           
CLIP_EPSILON = 0.2          
VALUE_LOSS_COEF = 0.5       
ENTROPY_COEF = 0.01         
MAX_GRAD_NORM = 0.5         
PPO_EPOCHS = 3             


def save_checkpoint(model, optimizer, iteration, log_dir, filename="checkpoint.pth"):
    """ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€"""
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        
    save_path = os.path.join(log_dir, filename)
    
    # æ‰“åŒ…æ‰€æœ‰éœ€è¦çš„ä¸œè¥¿
    state = {
        'iteration': iteration,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }
    torch.save(state, save_path)
    print(f"--> æ¨¡å‹å·²ä¿å­˜: {save_path}")

def load_checkpoint(model, optimizer, load_path):
    """åŠ è½½æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€"""
    print(f"--> æ­£åœ¨åŠ è½½æ¨¡å‹: {load_path}")
    checkpoint = torch.load(load_path,map_location="cuda:0")
    
    # æ¢å¤æ¨¡å‹å‚æ•°
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # æ¢å¤ä¼˜åŒ–å™¨ (å¦‚æœæ˜¯ç»§ç»­è®­ç»ƒï¼Œè¿™æ­¥å¾ˆé‡è¦)
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
    start_iter = checkpoint.get('iteration', 0)
    print(f"--> åŠ è½½æˆåŠŸï¼ä»ç¬¬ {start_iter} è½®ç»§ç»­è®­ç»ƒã€‚")
    return start_iter

device = "cuda:0"

def train():
    # ==========================================
    # 1. TensorBoard åˆå§‹åŒ– (æ–°å¢)
    # ==========================================
    # ç»™æ—¥å¿—ç›®å½•åŠ ä¸Šæ—¶é—´æˆ³ï¼Œé˜²æ­¢å¤šæ¬¡è®­ç»ƒçš„æ›²çº¿æ··åœ¨ä¸€èµ·
    time_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_dir = f"runs/CartPole_LSTM_{time_str}"
    writer = SummaryWriter(log_dir)
    print(f"TensorBoard æ—¥å¿—å°†ä¿å­˜åˆ°: {log_dir}")
    print(f"è¯·åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir=runs æ¥æŸ¥çœ‹å›¾è¡¨")

    # 2. åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹
    env = CartPoleEnv(num_envs=NUM_ENVS)
    model = CartPoleActorCritic(num_obs=5, num_actions=1).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    LOAD_PATH = "/home/gdp/second_stage/final_model.pth" # ä½ æƒ³åŠ è½½çš„æ–‡ä»¶è·¯å¾„
    resume = True  # ğŸ”´ å¼€å…³ï¼šTrue=æ¥ç€ç»ƒï¼ŒFalse=é‡å¤´ç»ƒ
    
    start_iter = 0
    if resume and os.path.exists(LOAD_PATH):
        start_iter = load_checkpoint(model, optimizer, LOAD_PATH)

    # åˆå§‹åŒ– hidden state
    hidden_state = (
        torch.zeros(1, NUM_ENVS, model.hidden_size).to(device),
        torch.zeros(1, NUM_ENVS, model.hidden_size).to(device)
    )

    print(f"å¼€å§‹è®­ç»ƒ! Device: {device}, Envs: {NUM_ENVS}")
    try:
        for iteration in range(MAX_ITERATIONS):
            # ... (Phase 1: Rollout ä»£ç ä¸å˜) ...
            buffer = {'obs': [], 'actions': [], 'log_probs': [], 'rewards': [], 'dones': [], 'values': []}
            
            initial_hidden = (hidden_state[0].detach(), hidden_state[1].detach())
            # .detach() æ„æ€æ˜¯ï¼š
            # "æŠŠä½ èº«ä¸Šçš„æ•°å€¼å¤åˆ¶ä¸€ä»½ç»™æˆ‘ï¼Œä½†æ˜¯æŠŠæ¢¯åº¦é“¾æ¡å‰ªæ–­ï¼"
            # ç°åœ¨çš„ initial_hidden å°±æ˜¯å•çº¯çš„æ•°å­—å¼ é‡ï¼Œæ²¡æœ‰ä»»ä½•å†å²åŒ…è¢±ã€‚
            hidden_state = initial_hidden 
            # 1. å‡†å¤‡æ‰€æœ‰ç¯å¢ƒçš„ ID
            all_env_ids = torch.ones(NUM_ENVS, dtype=torch.long, device=device)
            
            # 2. å¼ºåˆ¶å¤ä½æ‰€æœ‰ç¯å¢ƒ (ç‰©ç†å¤ä½ + éšæœº Command)
            env.reset(all_env_ids)

            obs = env.get_obs()

                # åˆå§‹åŒ– hidden state
            hidden_state = (
                torch.zeros(1, NUM_ENVS, model.hidden_size).to(device),
                torch.zeros(1, NUM_ENVS, model.hidden_size).to(device)
            )
            # print("Starting new iteration rollout...")
            epoch_reward_tracker = {
                'rew_angle': 0.0,
                'rew_vel': 0.0,
                'rew_stable': 0.0,
                'rew_action': 0.0,
                'raw_total': 0.0
            }
            for step in range(NUM_STEPS):
                with torch.no_grad(): #æ¥ä¸‹æ¥è¿™å‡ è¡Œä»£ç ï¼Œä½ åªç®¡ç®—ç»“æœï¼Œä¸è¦è®°å½•æ¢¯åº¦
                    action, log_prob, value, next_hidden = model.get_action(
                        obs.unsqueeze(0), hidden_state
                    )

                next_obs, reward, done,reward_info = env.step(action.squeeze(0),step,NUM_STEPS)

                buffer['obs'].append(obs)
                buffer['actions'].append(action.squeeze(0)) 
                buffer['log_probs'].append(log_prob.squeeze(0))
                buffer['values'].append(value.squeeze(0))
                buffer['rewards'].append(reward)
                buffer['dones'].append(done)
                for key in epoch_reward_tracker:
                    epoch_reward_tracker[key] += reward_info[key]

                obs = next_obs
                h, c = next_hidden
                mask = (1.0 - done.float()).view(1, -1, 1) 
                hidden_state = (h * mask, c * mask)
            # print("Rollout completed.")
            
            # ... (Phase 2: GAE ä»£ç ä¸å˜) ...

            # 1. å †å å¹¶ squeeze (å»é™¤æœ€åä¸€ä¸ªç»´åº¦å¦‚æœæ˜¯1çš„è¯)
            b_obs = torch.stack(buffer['obs'])          # [24, 512, 5]
            b_actions = torch.stack(buffer['actions'])  # [24, 512, 1] -> åŠ¨ä½œé€šå¸¸ä¿ç•™ç»´åº¦æ¯”è¾ƒå®‰å…¨ï¼Œçœ‹ä½ çš„åˆ†å¸ƒæ€ä¹ˆå†™çš„
            b_log_probs = torch.stack(buffer['log_probs']).squeeze() # [24, 512]
            
            # âš ï¸ å…³é”®ä¿®æ­£ï¼šæŠŠ values, rewards, dones å…¨éƒ¨æŒ¤å‹æˆä¸€ç»´
            b_rewards = torch.stack(buffer['rewards']).squeeze()     # [24, 512]
            b_dones = torch.stack(buffer['dones']).squeeze()         # [24, 512]
            b_values = torch.stack(buffer['values']).squeeze()       # [24, 512]

            # ï¼ˆè´å°”æ›¼æ–¹ç¨‹ï¼‰ï¼šå½“å‰ä»·å€¼ = å½“å‰å¥–åŠ± + æŠ˜æ‰£å› å­ * ä¸‹ä¸€æ­¥ä»·å€¼
            with torch.no_grad():
                _, _, next_value, _ = model.get_action(obs.unsqueeze(0), hidden_state)
                next_value = next_value.squeeze().to(device)


            
            # è®¡ç®— disconuntewd rewards(Gt) å’Œ GAE advantages
            # advantages -> action value
            advantages = torch.zeros_like(b_rewards).to(device)
            last_gae_lam = 0
            
            for t in reversed(range(NUM_STEPS)):
                if t == NUM_STEPS - 1:
                    next_non_terminal = 1.0 - 0.0 
                    next_val = next_value
                else:
                    # å› ä¸ºä¸Šé¢å·²ç» squeeze è¿‡äº†ï¼Œè¿™é‡Œ b_dones[t+1] ä¸€å®šæ˜¯ [512]
                    next_non_terminal = 1.0 - b_dones[t+1].float()
                    next_val = b_values[t+1]

                #TD Error
                delta = b_rewards[t] + GAMMA * next_val * next_non_terminal - b_values[t]            
                
                #Monte Carlo (è’™ç‰¹å¡æ´›) å’Œ TD(0) GAE æ˜¯è¿™ä¸¤ä¸ªçš„æ··è¡€å„¿
                last_gae_lam = delta + GAMMA * GAE_LAMBDA * next_non_terminal * last_gae_lam
                advantages[t] = last_gae_lam        

            #è¿™è¡Œä»£ç åªæ˜¯ä¸ªç®€å•çš„æ•°å­¦æ’ç­‰å¼å˜æ¢ï¼š$$\text{Return} = \text{Advantage} + \text{Value}$$
            # å› ä¸º Advantage çš„å®šä¹‰æœ¬æ¥å°±æ˜¯ï¼šâ€œå®é™…å›æŠ¥ ($Q$ æˆ– $R$) å‡å» é¢„æœŸä»·å€¼ ($V$)â€ã€‚
            returns = advantages + b_values   #returns -> value target



            # ... (Phase 3: PPO Update ä»£ç ä¸å˜) ...
            b_obs = b_obs.detach()
            b_actions = b_actions.detach()
            b_log_probs = b_log_probs.detach()
            b_advantages = advantages.detach()
            b_returns = returns.detach()
            
            # ç”¨æ¥è®°å½•è¿™ä¸€ä¸ª Batch çš„å¹³å‡ Lossï¼Œæ–¹ä¾¿ TensorBoard æ˜¾ç¤º
            avg_actor_loss = 0
            avg_value_loss = 0
            avg_entropy = 0
            # print("Starting PPO update...")
            for epoch in range(PPO_EPOCHS):
                # æ¯æ¬¡æ‰“ä¹±ç¯å¢ƒé¡ºåºï¼ˆè¿™æ˜¯ SGD çš„ç²¾é«“ï¼Œå¢åŠ éšæœºæ€§ï¼‰
                perm = torch.randperm(NUM_ENVS)
                
                # âœ… ä¿®æ­£ç‚¹ 1ï¼šæ­¥é•¿æ”¹ä¸º MINI_BATCH_SIZE
                # è¿™æ ·æ‰èƒ½çœŸæ­£æŠŠæ•°æ®åˆ‡æˆå°å—å–‚ç»™ GPU
                for i in range(0, NUM_ENVS, MINI_BATCH_SIZE):
                    
                    # âœ… ä¿®æ­£ç‚¹ 2ï¼šåˆ‡ç‰‡ç´¢å¼•
                    # Python çš„åˆ‡ç‰‡ä¼šè‡ªåŠ¨å¤„ç†æœ€åä¸è¶³ä¸€ä¸ª batch çš„æƒ…å†µï¼Œä¸ç”¨æ‹…å¿ƒè¶Šç•Œ
                    idxs = perm[i : i + MINI_BATCH_SIZE]
                    
                    # ---------------------------------------------------
                    # 1. åˆ‡åˆ†â€œè¿‡å»â€çš„æ•°æ® (Target)
                    # ---------------------------------------------------
                    # å‡è®¾ MINI_BATCH_SIZE = 512
                    # mb_obs: [24, 512, 5]
                    mb_obs = b_obs[:, idxs]           
                    mb_actions = b_actions[:, idxs]   
                    mb_log_probs = b_log_probs[:, idxs] 
                    mb_advantages = advantages[:, idxs] 
                    mb_returns = returns[:, idxs]       
                    
                    # ---------------------------------------------------
                    # 2. å¤„ç† LSTM çš„ Hidden State (åˆ‡åˆ†è¾“å…¥)
                    # ---------------------------------------------------
                    # initial_hidden æ˜¯ (h, c)ï¼Œå½¢çŠ¶æ˜¯ [1, NUM_ENVS, 256]
                    # æˆ‘ä»¬åªå–å½“å‰è¿™ 512 ä¸ªç¯å¢ƒå¯¹åº”çš„è®°å¿†
                    h_0 = initial_hidden[0][:, idxs]
                    c_0 = initial_hidden[1][:, idxs]
                    mb_hidden = (h_0, c_0)
                    
                    # ---------------------------------------------------
                    # 3. é‡æ–°è®¡ç®—â€œç°åœ¨â€çš„é¢„æµ‹ (Forward)
                    # ---------------------------------------------------
                    # æŠŠåˆ‡å¥½çš„å°æ‰¹é‡æ•°æ®å–‚ç»™æ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨å¤§å¤§é™ä½
                    # new_values è¾“å‡ºå½¢çŠ¶é€šå¸¸æ˜¯ [24, 512, 1] æˆ– [24, 512]
                    new_mean, new_std, new_values, _ = model(mb_obs, mb_hidden)
                    
                    # ---------------------------------------------------
                    # 4. è®¡ç®— Loss
                    # ---------------------------------------------------
                    # è¿™é‡Œçš„ dist æ‰æ˜¯ Policy Ï€(a|s) çš„æœ¬ä½“ï¼
                    dist = torch.distributions.Normal(new_mean, new_std)
                    new_log_probs = dist.log_prob(mb_actions).sum(dim=-1) 
                    
                    # KLæ•£åº¦ / ç†µï¼šmake policy more diverse
                    entropy = dist.entropy().sum(dim=-1).mean() 
                    
                    # Ratio = P_new / P_old
                    ratio = torch.exp(new_log_probs - mb_log_probs)
                    
                    surr1 = ratio * mb_advantages
                    surr2 = torch.clamp(ratio, 1.0 - CLIP_EPSILON, 1.0 + CLIP_EPSILON) * mb_advantages
                    actor_loss = -torch.min(surr1, surr2).mean()
                    
                    # âœ… ä¿®æ­£ç‚¹ 3ï¼šæ›´å®‰å…¨çš„ squeeze
                    # å»ºè®®ä½¿ç”¨ squeeze(-1) åªæŒ¤å‹æœ€åä¸€ä¸ªç»´åº¦ï¼Œé˜²æ­¢æŠŠ batch ç»´åº¦è¯¯æŒ¤å‹
                    # ç›®æ ‡ï¼šè®© new_values å’Œ mb_returns å½¢çŠ¶å®Œå…¨ä¸€è‡´
                    value_loss = 0.5 * ((new_values.squeeze(-1) - mb_returns) ** 2).mean()
                    
                    loss = actor_loss + VALUE_LOSS_COEF * value_loss - ENTROPY_COEF * entropy
                    
                    optimizer.zero_grad()
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢ LSTM è®­ç»ƒä¸­å¸¸è§çš„æ¢¯åº¦çˆ†ç‚¸
                    nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
                    
                    optimizer.step()

                # ç´¯åŠ  Loss
                avg_actor_loss += actor_loss.item()
                avg_value_loss += value_loss.item()
                avg_entropy += entropy.item()
            # print("PPO update completed.")
            # å–å¹³å‡
            avg_actor_loss /= PPO_EPOCHS
            avg_value_loss /= PPO_EPOCHS
            avg_entropy /= PPO_EPOCHS

            # =====================================================
            # 4. Logging with TensorBoard (å…³é”®ä¿®æ”¹)
            # =====================================================
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            total_steps = (iteration + 1) * NUM_STEPS * NUM_ENVS
            mean_reward = b_rewards.sum().item() / NUM_ENVS # æ¯ä¸ªç¯å¢ƒåœ¨è¿™24æ­¥é‡Œå¹³å‡æ‹¿äº†å¤šå°‘åˆ†
            total_failures = b_dones.sum().item() # è¿™ä¸€è½®é‡Œä¸€å…±å€’äº†å¤šå°‘æ¬¡è½¦
            
            # --- å†™æ—¥å¿— ---
            # 1. æ ¸å¿ƒè¡¨ç°
            writer.add_scalar('Performance/Mean_Reward', mean_reward, iteration)
            writer.add_scalar('Performance/Failures_Count', total_failures, iteration)
            
            # 2. æŸå¤±å‡½æ•° (ç”¨æ¥è¯Šæ–­ç½‘ç»œæ˜¯å¦åœ¨å­¦ä¹ )
            writer.add_scalar('Loss/J1', -avg_actor_loss, iteration)
            writer.add_scalar('Loss/Value_Loss', avg_value_loss, iteration)
            writer.add_scalar('Loss/Entropy', avg_entropy, iteration) # ç†µè¶Šä½ï¼Œç­–ç•¥è¶Šç¡®å®šï¼›ç†µè¶Šé«˜ï¼Œè¶Šéšæœº
            
            # 3. ç­–ç•¥å‚æ•° (è§‚å¯Ÿ STD å˜åŒ–å¾ˆæœ‰æ„æ€ï¼Œçœ‹å®ƒæ˜¯ä¸æ˜¯åœ¨å˜å°)
            # æˆ‘ä»¬å–æ‰€æœ‰ç¯å¢ƒ STD çš„å¹³å‡å€¼
            current_std = model.actor_log_std.exp().mean().item()
            writer.add_scalar('Policy/Action_Std', current_std, iteration)


            for key, total_value in epoch_reward_tracker.items():
                avg_value = total_value / NUM_STEPS  # ç®—å‡ºå¹³å‡æ¯ä¸€æ­¥æ‹¿å¤šå°‘åˆ†
                writer.add_scalar(f'Rewards/{key}', avg_value, iteration)

            # ç»ˆç«¯æ‰“å°ç®€åŒ–ï¼Œè¯¦ç»†çš„å» TensorBoard çœ‹
            if iteration % SAVE_INTERVAL == 0:
                print(f"Iter {iteration}: Reward={mean_reward:.2f}, Failures={int(total_failures)}, Std={current_std:.2f}")
                save_checkpoint(model, optimizer, iteration, log_dir, "latest_model.pth")
            
            # if iteration % 10 == 0:
            #     print(f"Iter {iteration}: Reward={mean_reward:.2f}, Failures={int(total_failures)}, Std={current_std:.2f}")

            # if avg_reward > 450: # å‡è®¾æ»¡åˆ† 500
            #             save_checkpoint(model, optimizer, iteration, log_dir, f"best_reward_{int(avg_reward)}.pth")
    except KeyboardInterrupt:
        print("\næ£€æµ‹åˆ° Ctrl+C!æ­£åœ¨ç´§æ€¥ä¿å­˜æ¨¡å‹...")
        save_checkpoint(model, optimizer, iteration, log_dir, "interrupted_model.pth")
        print("ä¿å­˜å®Œæ¯•ï¼Œå®‰å…¨é€€å‡ºã€‚")
        
    except Exception as e:
        print(f"\nå‘ç”Ÿé”™è¯¯: {e}")
        # å‡ºé”™ä¹Ÿå°è¯•ä¿å­˜ä¸€ä¸‹
        save_checkpoint(model, optimizer, iteration, log_dir, "crash_model.pth")
        raise e

    # ç»“æŸæ—¶å…³é—­ writer
    writer.close()
    save_checkpoint(model, optimizer, MAX_ITERATIONS, log_dir, "final_model.pth")
    print("Training Finished!")

if __name__ == "__main__":
    train()