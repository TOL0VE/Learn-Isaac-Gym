from isaacgym import gymapi, gymtorch
import torch
import numpy as np

class CartPoleEnv:
    def __init__(self, num_envs=512):
        self.control_steps = 2  # æ¯ä¸ªåŠ¨ä½œæ‰§è¡Œå¤šå°‘ä¸ªç‰©ç†æ­¥é•¿

        self.gym = gymapi.acquire_gym()
        self.sim_params = gymapi.SimParams()
        self.sim_params.physx.use_gpu = True
        self.sim_params.up_axis = gymapi.UP_AXIS_Z
        self.sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.81)
        self.sim_params.dt = 0.01
        # âœ… åŠ¡å¿…åŠ ä¸Šè¿™è¡Œï¼è®©ç‰©ç†å¼•æ“çš„æ•°æ®ç›´æ¥ç•™åœ¨ GPU ä¸Š
        self.sim_params.use_gpu_pipeline = True
        # å€ç‡æ‰©å®¹
        # é»˜è®¤æ˜¯ 1ã€‚è®¾ä¸º 4 æˆ– 8ï¼Œä¼šæˆå€å¢åŠ æ‰€æœ‰ç‰©ç†ç¼“å†²åŒºçš„é¢„åˆ†é…å¤§å°
        self.sim_params.physx.default_buffer_size_multiplier = 4.0 
        # 2. å¢åŠ æœ€å¤§ GPU æ¥è§¦å¯¹æ•°é‡ (ä»¥é˜²ä¸‡ä¸€)
        # é»˜è®¤é€šå¸¸æ˜¯ 1024*1024 (1M)ã€‚ç»™å®ƒåŠ åˆ° 8M æˆ– 16M
        self.sim_params.physx.max_gpu_contact_pairs = 8 * 1024 * 1024 
        # 3. å¢åŠ æ¥è§¦ç‚¹ç¼“å†² (ä¹Ÿæ˜¯ä»¥é˜²ä¸‡ä¸€)
        self.sim_params.physx.max_gpu_contact_pairs = 8 * 1024 * 1024
        
        # é€‰æ‹©æ˜¾å¡
        self.device = "cuda:0"
        self.sim = self.gym.create_sim(0, 0, gymapi.SIM_PHYSX, self.sim_params)

        self.viewer = None
        # self.viewer = self.gym.create_viewer(self.sim, gymapi.CameraProperties())
        
        # 1. åŠ è½½ CartPole èµ„äº§
        asset_root = "/home/oiioaa/Desktop/isaacgym/assets"
        asset_file = "urdf/cartpole.urdf" # Isaac Gym è‡ªå¸¦è¿™ä¸ªæ–‡ä»¶
        asset_options = gymapi.AssetOptions()
        asset_options.fix_base_link = True # æŠŠæ»‘è½¨å›ºå®šåœ¨ç©ºä¸­
        cartpole_asset = self.gym.load_asset(self.sim, asset_root, asset_file, asset_options)
        
        # 2. åˆ›å»ºç¯å¢ƒ
        self.num_envs = num_envs
        self.envs = []
        self.actor_handles = []
        
        env_spacing = 2.0
        lower = gymapi.Vec3(-env_spacing, -env_spacing, 0.0)
        upper = gymapi.Vec3(env_spacing, env_spacing, env_spacing)
        
        print(f"æ­£åœ¨åˆ›å»º {num_envs} ä¸ª CartPole...")
        for i in range(self.num_envs):
            env = self.gym.create_env(self.sim, lower, upper, int(np.sqrt(self.num_envs)))
            handle = self.gym.create_actor(env, cartpole_asset, gymapi.Transform(), "cartpole", i, 1)
            
            # è®¾ç½®è‡ªç”±åº¦å±æ€§ (Cart æ˜¯é©±åŠ¨çš„ï¼ŒPole æ˜¯è‡ªç”±æ‘†åŠ¨çš„)
            props = self.gym.get_actor_dof_properties(env, handle)
            props['driveMode'][0] = gymapi.DOF_MODE_EFFORT # å°è½¦ç”¨åŠ›æ§åˆ¶
            props['driveMode'][1] = gymapi.DOF_MODE_NONE   # æ‘†æ†è‡ªç”±
            props['stiffness'][:] = 0.0
            props['damping'][:] = 0.0
            self.gym.set_actor_dof_properties(env, handle, props)
            
            self.envs.append(env)
            self.actor_handles.append(handle)
            
        # 3. å‡†å¤‡ Tensor æ¥å£ (GPUåŠ é€Ÿ)
        self.gym.prepare_sim(self.sim)
        
        # è·å–çŠ¶æ€ Tensor (num_envs, 2) -> (ä½ç½®, é€Ÿåº¦)
        _dof_states = self.gym.acquire_dof_state_tensor(self.sim)#è‹¥æƒ³è·å¾—æ›´å…¨é¢çš„çŠ¶æ€ä¿¡æ¯ï¼Œå¯ä»¥ä½¿ç”¨ acquire_actor_root_state_tensor
        self.dof_states = gymtorch.wrap_tensor(_dof_states)
        # Cartä½ç½®, Carté€Ÿåº¦, Poleè§’åº¦, Poleè§’é€Ÿåº¦
        # shape: (num_envs, 4) å› ä¸ºæ¯ä¸ªç¯å¢ƒæœ‰2ä¸ªå…³èŠ‚(Cart, Pole)ï¼Œæ¯ä¸ªå…³èŠ‚æœ‰poså’Œvel
        self.root_states = self.dof_states.view(self.num_envs, 4) 
        
        # ç›®æ ‡æŒ‡ä»¤ (Target Velocity) - ç”¨æ¥å­˜é”®ç›˜æŒ‡ä»¤
        self.commands = torch.zeros(self.num_envs, 1, device=self.device)

    def get_obs(self):
        self.gym.refresh_dof_state_tensor(self.sim)
        
        # è§‚æµ‹ = [CartPos, CartVel, PoleAngle, PoleVel, Command]
        # æ³¨æ„ï¼šCartPos ç´¢å¼•æ˜¯ 0, CartVel æ˜¯ 1, PoleAngle æ˜¯ 2, PoleVel æ˜¯ 3 (viewä¹‹å)
        
        # å½’ä¸€åŒ–/ç¼©æ”¾é€šå¸¸åœ¨è¿™é‡Œåšï¼Œä¸ºäº†ç®€å•å…ˆç›´æ¥ä¼ 
        obs = torch.cat([self.root_states, self.commands], dim=1)
        return obs

    def step(self, actions,step,NUM_STEPS):
        # actions: (num_envs, 1) -> åŠ›çŸ©
        # éœ€è¦è½¬æ¢æˆ tensor æ ¼å¼å–‚ç»™ Isaac Gym
        forces = torch.zeros((self.num_envs, 2), device=self.device, dtype=torch.float32)
        forces[:, 0] = actions.squeeze() * 100.0 # æ”¾å¤§åŠ›çŸ©ï¼Œå¦åˆ™æ¨ä¸åŠ¨
        # print(f"Action: {actions.squeeze().cpu().numpy()* 100.0}")
        
        # æ–½åŠ åŠ›çŸ©
        self.gym.set_dof_actuation_force_tensor(self.sim, gymtorch.unwrap_tensor(forces))
        
        for _ in range(self.control_steps):
            self.gym.simulate(self.sim)
            self.gym.fetch_results(self.sim, True)

            # æ¸²æŸ“é€»è¾‘ä¹Ÿå¯ä»¥æ”¾åœ¨è¿™
            if self.viewer:
                self.gym.step_graphics(self.sim)
                self.gym.draw_viewer(self.viewer, self.sim, True)
                self.gym.sync_frame_time(self.sim)
        
        # è®¡ç®— Reward (å…³é”®ï¼)
        # ç›®æ ‡ï¼š
        # 1. æ†å­ç«–ç›´ (PoleAngle -> 0)
        # 2. å°è½¦é€Ÿåº¦è¿½è¸ªæŒ‡ä»¤ (CartVel -> Command)
        
        # --- 1. è·å–çŠ¶æ€ ---
        # å‡è®¾ root_states é¡ºåºæ˜¯ [cart_pos, cart_vel, pole_angle, pole_vel]
        cart_pos = self.root_states[:, 0]
        cart_vel = self.root_states[:, 1]
        pole_angle = self.root_states[:, 2]
        pole_vel = self.root_states[:, 3]
        target_vel = self.commands.squeeze()
        wall_limit = 2.4
        soft_limit = 2.1
        safe_target_vel = target_vel.clone()
        # æƒ…å†µ A: é å³å¢™å¤ªè¿‘ (> 2.1)
        # æ­¤æ—¶ä¸ç®¡åŸå§‹æŒ‡ä»¤æƒ³è¦å¾€å“ªè·‘ï¼Œå¼ºåˆ¶æ”¹æˆâ€œå¾€å·¦å›ä¸­â€ (æ¯”å¦‚ -1.0)
        # æˆ–è€…æŒ‰ä½ çš„è¦æ±‚ï¼Œæ”¹æˆ 0 (åœè½¦)ï¼Œä½†åœ¨è¾¹ç•Œåœè½¦ä¸å¦‚å›å¤´å®‰å…¨
        # å»ºè®®ï¼šå¼ºåˆ¶ç»™ä¸€ä¸ªåå‘é€Ÿåº¦ï¼ŒæŠŠè½¦â€œæ¨â€å›æ¥
        right_danger = cart_pos > soft_limit
        safe_target_vel = torch.where(right_danger, -1.0 * torch.ones_like(safe_target_vel), safe_target_vel)
        
        # æƒ…å†µ B: é å·¦å¢™å¤ªè¿‘ (< -2.1)
        # å¼ºåˆ¶æ”¹æˆâ€œå¾€å³å›ä¸­â€ (æ¯”å¦‚ +1.0)
        left_danger = cart_pos < -soft_limit
        safe_target_vel = torch.where(left_danger, 1.0 * torch.ones_like(safe_target_vel), safe_target_vel)

        # ğŸ’¡ å¦‚æœåªæ˜¯æƒ³å˜ 0
        # safe_target_vel = torch.where(right_danger | left_danger, torch.zeros_like(safe_target_vel), safe_target_vel)

        
        # --- 2. å®šä¹‰æƒ©ç½šé¡¹ (Penalties) ---
        
        # --- 1. é¢„å¤„ç†ï¼šè§’åº¦å½’ä¸€åŒ– (Swing-Up å¿…åšï¼) ---
        # Isaac Gym çš„ pole_angle æ˜¯ç´¯ç§¯çš„ (æ¯”å¦‚è½¬ä¸€åœˆæ˜¯ 6.28)ã€‚
        # å¯¹äºå¹³æ–¹æƒ©ç½šï¼Œå¿…é¡»æŠŠè§’åº¦â€œæŠ˜å â€å› [-Ï€, Ï€] åŒºé—´ã€‚
        # å¦åˆ™ï¼šæ†å­ç”©ä¸Šå»å˜æˆ 2Ï€ (çº¦6.28)ï¼Œå¹³æ–¹åæ˜¯ 40ï¼Œæƒ©ç½šçˆ†è¡¨ï¼ŒAI å°±ä¸æ•¢ç”©äº†ã€‚
        pole_angle_wrapped = (pole_angle + torch.pi) % (2 * torch.pi) - torch.pi
        
        # --- 2. å®šä¹‰æƒ©ç½šé¡¹ ---
        
        # A. è§’åº¦æƒ©ç½š (ä½¿ç”¨æŠ˜å åçš„è§’åº¦)
        # ç›®æ ‡æ˜¯è®© wrapped è§’åº¦å½’ 0
        r_angle = -5.0 * (pole_angle_wrapped ** 2)
        
        # B. é€Ÿåº¦æƒ©ç½š
        r_vel = -0.5 * ((cart_vel - safe_target_vel) ** 2)
        
        # C. ç¨³å®šæ€§æƒ©ç½š
        r_pole_stable = -0.05 * (pole_vel ** 2)
        
        # D. åŠ¨ä½œæƒ©ç½š
        r_action = -0.01 * (actions.squeeze() ** 2)

        # ä½ç½®æƒ©ç½š (Position Penalty)
        # x=0æ—¶ä¸æ‰£åˆ†ï¼Œx=2.0æ—¶æ‰£ -4.0 åˆ†
        # è¿™åƒä¸€æ ¹æ©¡çš®ç­‹ï¼ŒæŠŠå®ƒå¾€ä¸­é—´æ‹‰
        r_pos = -1.0 * (self.root_states[:, 0] ** 2)

        # --- 3. å®‰å…¨åŒºé®ç½© (Masking) ---
        # åªæœ‰å½“æ†å­æ¯”è¾ƒç›´ (Â±0.4 rad, çº¦24åº¦) æ—¶ï¼Œæ‰å¼€å§‹è€ƒè™‘é€Ÿåº¦å’ŒçœåŠ›
        # ç”©æ†è¿‡ç¨‹ä¸­(ä¸å®‰å…¨æ—¶)ï¼Œåªä¸“æ³¨äº r_angleï¼Œå…¶ä»–å¿½ç•¥
        is_safe = torch.abs(pole_angle_wrapped) < 0.4 
        
        r_vel = torch.where(is_safe, r_vel, torch.zeros_like(r_vel))
        r_pole_stable = torch.where(is_safe, r_pole_stable, torch.zeros_like(r_pole_stable))
        r_action = torch.where(is_safe, r_action, torch.zeros_like(r_action))
        
        # --- 4. è®¡ç®—æ€»å¥–åŠ± ---
        # âœ… åŠ  1.0 æ´»ç€å¥–åŠ±ï¼Œé˜²æ­¢è´Ÿåˆ†å¤ªå¤šå¯¼è‡´ AI è‡ªæ€
        reward = 1.0 + r_angle + r_pole_stable + r_vel + r_action + r_pos
        
        # --- 5. å¤±è´¥åˆ¤å®š (Reset Logic) ---
        
        # A. è§’åº¦é˜ˆå€¼ (Swing-Up ä»»åŠ¡é€šå¸¸ä¸è®¾è§’åº¦é‡ç½®ï¼Œæˆ–è€…è®¾å¾ˆå¤§)
        # è®¾ä¸º 2*PI å…è®¸å®ƒç”©ä¸€åœˆï¼›å¦‚æœè®¾å¤ªå°(3.14)å¯èƒ½ä¼šæ‰“æ–­ç”©æ†çš„åŠ¨ä½œ
        reset_threshold = 2 * 3.14 
        pole_failed = torch.abs(pole_angle) > reset_threshold
        
        # B. å‡ºç•Œåˆ¤å®š (ç»å¯¹ä¸èƒ½å¿)
        out_of_bounds = torch.abs(self.root_states[:, 0]) > 2.4
        
        # åˆå¹¶æ‰€æœ‰å¤±è´¥æ¡ä»¶
        reset_env_ids = pole_failed | out_of_bounds

        # --- 6. æ­»äº¡æƒ©ç½š (å…³é”®ä¿®æ­£é¡ºåº) ---
        # âŒ åŸä»£ç ï¼šå‡ºç•Œ(out_of_bounds)æ²¡æœ‰è¢«åŒ…å«åœ¨æƒ©ç½šé‡Œï¼ŒAI ä¼šæ•…æ„å‡ºç•Œæ¥éª—åˆ†ã€‚
        # âœ… æ–°ä»£ç ï¼šåªè¦éœ€è¦ Reset (æ— è®ºæ˜¯å€’äº†è¿˜æ˜¯å‡ºç•Œ)ï¼Œç»Ÿç»Ÿæ‰£åˆ†ã€‚
        
        # å¦‚æœæ˜¯æœ€åä¸€æ­¥è‡ªç„¶åœæ­¢ï¼Œä¸æ‰£åˆ†ï¼›å¦åˆ™æ‰£ 20 åˆ†
        # (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç»Ÿä¸€æ‰£åˆ†ï¼Œæ•ˆæœé€šå¸¸æ›´ç¨³)
        penalty_mask = reset_env_ids & (step < NUM_STEPS - 1)
        reward = torch.where(out_of_bounds, reward - 100.0, reward)
        reward = torch.where(penalty_mask, reward - 20.0, reward)
        
        # --- 7. æ‰§è¡Œ Reset ---
        if torch.any(reset_env_ids):
            self.reset(reset_env_ids)

        # è¿”å› Info (ç”¨äº TensorBoard è°ƒè¯•)
        reward_info = {
            'rew_angle': r_angle.mean().item(),
            'rew_vel': r_vel.mean().item(),
            'rew_stable': r_pole_stable.mean().item(),
            'rew_action': r_action.mean().item(),
            'raw_total': reward.mean().item(),
            'rew_pos': r_pos.mean().item()
        }
        
        return self.get_obs(), reward, reset_env_ids, reward_info

    def reset(self, env_ids):
            # env_ids æ˜¯ä¸€ä¸ª bool tensor (æ¯”å¦‚ [False, True, False...])
            # 1. è·å–éœ€è¦é‡ç½®çš„ç¯å¢ƒç´¢å¼• (å˜æˆ [1, 5, 8...] è¿™ç§æ•´æ•°ç´¢å¼•)
            indices = env_ids.nonzero(as_tuple=False).flatten()
            num_resets = len(indices)
            if num_resets == 0: return

            # 2. ç”Ÿæˆéšæœºåˆå§‹çŠ¶æ€ (åªé’ˆå¯¹éœ€è¦é‡ç½®çš„é‚£å‡ ä¸ª)


            # 3. æ›´æ–° Tensor è§†å›¾ (æœ€å…³é”®çš„ä¸€æ­¥)
            # è¿˜è®°å¾—æˆ‘ä»¬åœ¨ __init__ é‡Œåšçš„é‚£ä¸ª .view() å—ï¼Ÿ
            # self.root_states æ˜¯ self.dof_states çš„ä¸€ä¸ªâ€œé©¬ç”²â€ã€‚
            # ä¿®æ”¹ self.root_states ä¼šç›´æ¥ä¿®æ”¹ self.dof_states çš„å†…å­˜ï¼
            
            # èµ‹å€¼é€»è¾‘ï¼š[indices] æŒ‘å‡ºç‰¹å®šè¡Œï¼Œ[0/1/2/3] æŒ‘å‡ºç‰¹å®šåˆ—
            self.root_states[indices, 0] =  0.0  # Cart Position
            self.root_states[indices, 1] =  (torch.rand((num_resets), device=self.device) - 0.5) * 2 * 2.0 # Cart Velocity
            self.root_states[indices, 2] =  (torch.rand((num_resets), device=self.device) - 0.5) * 2 * 3.14  # Pole Angle
            self.root_states[indices, 3] =  (torch.rand((num_resets), device=self.device) - 0.5) * 2 * 3.14  # Pole Velocity

            # 4. é€šçŸ¥ç‰©ç†å¼•æ“ (å¿…é¡»ç”¨ Int32 ç±»å‹)
            actor_indices = indices.to(dtype=torch.int32)
            
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼š
            # "æˆ‘ä¿®æ”¹äº† self.dof_states è¿™ä¸ªå¤§è¡¨ï¼Œä½†æˆ‘åªå¸Œæœ›ä½ æ›´æ–° actor_indices è¿™äº›è¡Œ"
            self.gym.set_dof_state_tensor_indexed(
                self.sim,
                gymtorch.unwrap_tensor(self.dof_states), # ä¼ å…¥æ•´ä¸ªå¤§è¡¨
                gymtorch.unwrap_tensor(actor_indices),   # ä¼ å…¥è¦æ›´æ–°çš„ç´¢å¼•åˆ—è¡¨
                num_resets                               # ä¼ å…¥æ›´æ–°çš„æ•°é‡
            )
            
            # 5. åŒæ—¶ä¹Ÿè¦æŠŠ Command (ç›®æ ‡é€Ÿåº¦) é‡ç½®ä¸€ä¸‹ï¼Œé˜²æ­¢åˆšå‡ºç”Ÿå°±å¸¦ç€æ—§æŒ‡ä»¤
            # è¿™é‡Œçš„é€»è¾‘å¯ä»¥è‡ªå®šä¹‰ï¼Œæ¯”å¦‚é‡ç½®ä¸º 0ï¼Œæˆ–è€…éšæœºç”Ÿæˆä¸€ä¸ªæ–°çš„ç›®æ ‡
            # self.commands[indices] = 0.0

            # # 1. ç›´æ¥åœ¨ GPU ä¸Šç”Ÿæˆ 0, 1, 2 (å½¢çŠ¶ç›´æ¥å¯¹é½ï¼Œä¸éœ€è¦ unsqueeze)
            # # high=3 æ„å‘³ç€å–å€¼èŒƒå›´æ˜¯ [0, 3)ï¼Œå³ 0, 1, 2
            # rand_ints = torch.rand(low=0, high=3, size=(num_resets, 1), device=self.device)
            # # 2. è½¬æ¢æˆ float å¹¶å‡å» 1 -> å˜æˆ -1.0, 0.0, 1.0
            # self.commands[indices] = rand_ints.float() - 1.0
            self.commands[indices] = (torch.rand((num_resets, 1), device=self.device) - 0.5) * 2 * 0
